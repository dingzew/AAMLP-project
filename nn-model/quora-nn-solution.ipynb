{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "WNL = WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = \"memento\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FOLDS = 10\n",
    "BATCH_SIZE = 1025\n",
    "EMBEDDING_FILE = \"../glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
    "\n",
    "def preprocess(string):\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "def prepare(q):\n",
    "    new_q = []      # New question after processing\n",
    "    surplus_q = []  # set of words neither belong to [topwords, STOPWORDS, digits]\n",
    "    numbers_q = []  # set of digits\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:  # Why in reverse order???\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif w not in STOP_WORDS:\n",
    "            if new_memento:\n",
    "                new_q = [\"memento\"] + new_q\n",
    "                new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)\n",
    "\n",
    "def extract_features(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "\n",
    "    for i, (q1, q2) in enumerate(list(zip(df[\"question1\"], df[\"question2\"]))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "\n",
    "    return q1s, q2s, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = 'quora'\n",
    "data_dir, feature_dir = '../quora/', '../quora/'\n",
    "os.mkdir('quora-models')\n",
    "os.mkdir('quora-predictions')\n",
    "\n",
    "# dataset = 'wiki'\n",
    "# data_dir, feature_dir = '../wiki/', '../wiki/'\n",
    "# os.mkdir('wiki-models')\n",
    "# os.mkdir('wiki-predictions')\n",
    "\n",
    "train = pd.read_csv(data_dir + \"train.csv\")\n",
    "\n",
    "train[\"question1\"] = train[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "train[\"question2\"] = train[\"question2\"].fillna(\"\").apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 100\n",
      "Words are not found in the embedding: {'quorans', 'redmi', 'kvpy', 'oneplus', 'demonetisation', 'iisc', 'paytm', 'c#', '\\\\sqrt', '\\\\frac', 'brexit'}\n",
      "Train questions are being prepared for LSTM...\n",
      "Train features are being merged with NLP and Non-NLP features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "all_questions = pd.Series(train[\"question1\"].tolist() + train[\"question2\"].tolist()).unique()\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(all_questions)\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)\n",
    "\n",
    "embeddings_index = get_embedding()\n",
    "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()\n",
    "\n",
    "print(\"Train questions are being prepared for LSTM...\")\n",
    "q1s_train, q2s_train, train_q_features = extract_features(train)\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train[\"is_duplicate\"])\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"Train features are being merged with NLP and Non-NLP features...\")\n",
    "train_nlp_features = pd.read_csv(feature_dir + \"nlp_features_train.csv\")\n",
    "train_non_nlp_features = pd.read_csv(feature_dir + \"non_nlp_features_train.csv\")\n",
    "features_train = np.hstack((train_q_features, train_nlp_features, train_non_nlp_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split 10% of train as valid to analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/20\n",
      " - 55s - loss: 0.2716 - val_loss: 0.2361\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2375 - val_loss: 0.2303\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2251 - val_loss: 0.2126\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2160 - val_loss: 0.2083\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2090 - val_loss: 0.2053\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2032 - val_loss: 0.2112\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1980 - val_loss: 0.2020\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1937 - val_loss: 0.2005\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1891 - val_loss: 0.2013\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1850 - val_loss: 0.2000\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1819 - val_loss: 0.2003\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1790 - val_loss: 0.2002\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1760 - val_loss: 0.1996\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1737 - val_loss: 0.2008\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1707 - val_loss: 0.2019\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1679 - val_loss: 0.2024\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1663 - val_loss: 0.2047\n",
      "Epoch 18/20\n",
      " - 52s - loss: 0.1651 - val_loss: 0.2022\n",
      "0 validation loss: 0.1996282116604496\n",
      "40429/40429 [==============================] - 3s 63us/step\n",
      "MODEL: 1\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/20\n",
      " - 56s - loss: 0.2725 - val_loss: 0.2398\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2384 - val_loss: 0.2268\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2255 - val_loss: 0.2226\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2166 - val_loss: 0.2150\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2095 - val_loss: 0.2099\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2034 - val_loss: 0.2082\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1982 - val_loss: 0.2084\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1928 - val_loss: 0.2074\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1893 - val_loss: 0.2042\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1860 - val_loss: 0.2036\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1825 - val_loss: 0.2067\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1790 - val_loss: 0.2024\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1765 - val_loss: 0.2048\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1729 - val_loss: 0.2044\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1708 - val_loss: 0.2110\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1691 - val_loss: 0.2075\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1659 - val_loss: 0.2087\n",
      "1 validation loss: 0.2024476733422651\n",
      "40429/40429 [==============================] - 3s 65us/step\n",
      "MODEL: 2\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/20\n",
      " - 55s - loss: 0.2737 - val_loss: 0.2365\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2373 - val_loss: 0.2249\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2247 - val_loss: 0.2194\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2160 - val_loss: 0.2115\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2092 - val_loss: 0.2132\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2033 - val_loss: 0.2066\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1975 - val_loss: 0.2089\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1929 - val_loss: 0.2076\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1894 - val_loss: 0.2064\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1857 - val_loss: 0.2049\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1816 - val_loss: 0.2061\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1793 - val_loss: 0.2034\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1763 - val_loss: 0.2055\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1733 - val_loss: 0.2046\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1710 - val_loss: 0.2049\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1684 - val_loss: 0.2068\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1662 - val_loss: 0.2067\n",
      "2 validation loss: 0.20340068685787968\n",
      "40429/40429 [==============================] - 3s 68us/step\n",
      "MODEL: 3\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/20\n",
      " - 55s - loss: 0.2712 - val_loss: 0.2460\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2371 - val_loss: 0.2352\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2238 - val_loss: 0.2195\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2153 - val_loss: 0.2239\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2083 - val_loss: 0.2133\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2028 - val_loss: 0.2128\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1986 - val_loss: 0.2107\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1934 - val_loss: 0.2122\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1901 - val_loss: 0.2114\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1852 - val_loss: 0.2078\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1824 - val_loss: 0.2058\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1796 - val_loss: 0.2065\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1763 - val_loss: 0.2068\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1731 - val_loss: 0.2080\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1705 - val_loss: 0.2075\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1686 - val_loss: 0.2090\n",
      "3 validation loss: 0.20584435749221694\n",
      "40429/40429 [==============================] - 3s 70us/step\n",
      "MODEL: 4\n",
      "Train on 327475 samples, validate on 36386 samples\n",
      "Epoch 1/20\n",
      " - 56s - loss: 0.2712 - val_loss: 0.2371\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2375 - val_loss: 0.2216\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2248 - val_loss: 0.2142\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2159 - val_loss: 0.2113\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2088 - val_loss: 0.2102\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2032 - val_loss: 0.2071\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1976 - val_loss: 0.2051\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1933 - val_loss: 0.2066\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1891 - val_loss: 0.2044\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1855 - val_loss: 0.2043\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1816 - val_loss: 0.2042\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1792 - val_loss: 0.2047\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1759 - val_loss: 0.2037\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1734 - val_loss: 0.2050\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1704 - val_loss: 0.2056\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1683 - val_loss: 0.2041\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1658 - val_loss: 0.2083\n",
      "Epoch 18/20\n",
      " - 52s - loss: 0.1635 - val_loss: 0.2083\n",
      "4 validation loss: 0.2037066844431379\n",
      "40429/40429 [==============================] - 3s 73us/step\n",
      "MODEL: 5\n",
      "Train on 327475 samples, validate on 36386 samples\n",
      "Epoch 1/20\n",
      " - 56s - loss: 0.2715 - val_loss: 0.2384\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2376 - val_loss: 0.2259\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2245 - val_loss: 0.2145\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2154 - val_loss: 0.2099\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2082 - val_loss: 0.2063\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2025 - val_loss: 0.2069\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1979 - val_loss: 0.2035\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1926 - val_loss: 0.2029\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1896 - val_loss: 0.2013\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1851 - val_loss: 0.2052\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1813 - val_loss: 0.2019\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1785 - val_loss: 0.2008\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1759 - val_loss: 0.2027\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1729 - val_loss: 0.2036\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1705 - val_loss: 0.2009\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1687 - val_loss: 0.2018\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1663 - val_loss: 0.2003\n",
      "Epoch 18/20\n",
      " - 52s - loss: 0.1638 - val_loss: 0.2032\n",
      "Epoch 19/20\n",
      " - 52s - loss: 0.1620 - val_loss: 0.2042\n",
      "Epoch 20/20\n",
      " - 52s - loss: 0.1601 - val_loss: 0.2045\n",
      "5 validation loss: 0.20030859244075858\n",
      "40429/40429 [==============================] - 3s 80us/step\n",
      "MODEL: 6\n",
      "Train on 327475 samples, validate on 36386 samples\n",
      "Epoch 1/20\n",
      " - 56s - loss: 0.2709 - val_loss: 0.2524\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2370 - val_loss: 0.2281\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2245 - val_loss: 0.2246\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2154 - val_loss: 0.2172\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2084 - val_loss: 0.2223\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2029 - val_loss: 0.2105\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1979 - val_loss: 0.2077\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1921 - val_loss: 0.2090\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1891 - val_loss: 0.2090\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1850 - val_loss: 0.2076\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1815 - val_loss: 0.2068\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1786 - val_loss: 0.2084\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1751 - val_loss: 0.2088\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1727 - val_loss: 0.2059\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1702 - val_loss: 0.2075\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1677 - val_loss: 0.2092\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1657 - val_loss: 0.2074\n",
      "Epoch 18/20\n",
      " - 52s - loss: 0.1638 - val_loss: 0.2102\n",
      "Epoch 19/20\n",
      " - 52s - loss: 0.1620 - val_loss: 0.2109\n",
      "6 validation loss: 0.20591575737543172\n",
      "40429/40429 [==============================] - 3s 81us/step\n",
      "MODEL: 7\n",
      "Train on 327476 samples, validate on 36385 samples\n",
      "Epoch 1/20\n",
      " - 57s - loss: 0.2724 - val_loss: 0.2396\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2375 - val_loss: 0.2284\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2250 - val_loss: 0.2177\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2152 - val_loss: 0.2151\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2079 - val_loss: 0.2155\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2026 - val_loss: 0.2104\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1972 - val_loss: 0.2062\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1929 - val_loss: 0.2070\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1886 - val_loss: 0.2051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      " - 52s - loss: 0.1850 - val_loss: 0.2040\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1814 - val_loss: 0.2076\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1785 - val_loss: 0.2080\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1748 - val_loss: 0.2085\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1727 - val_loss: 0.2071\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1702 - val_loss: 0.2059\n",
      "7 validation loss: 0.2040382142277132\n",
      "40429/40429 [==============================] - 3s 83us/step\n",
      "MODEL: 8\n",
      "Train on 327476 samples, validate on 36385 samples\n",
      "Epoch 1/20\n",
      " - 57s - loss: 0.2743 - val_loss: 0.2472\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2386 - val_loss: 0.2197\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2253 - val_loss: 0.2136\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2171 - val_loss: 0.2084\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2101 - val_loss: 0.2071\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2046 - val_loss: 0.2069\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1992 - val_loss: 0.2010\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1950 - val_loss: 0.2006\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1898 - val_loss: 0.2001\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1862 - val_loss: 0.1992\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1829 - val_loss: 0.2045\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1797 - val_loss: 0.1999\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1767 - val_loss: 0.2001\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1745 - val_loss: 0.1998\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1713 - val_loss: 0.2043\n",
      "8 validation loss: 0.19922970332613812\n",
      "40429/40429 [==============================] - 4s 87us/step\n",
      "MODEL: 9\n",
      "Train on 327476 samples, validate on 36385 samples\n",
      "Epoch 1/20\n",
      " - 57s - loss: 0.2716 - val_loss: 0.2453\n",
      "Epoch 2/20\n",
      " - 52s - loss: 0.2370 - val_loss: 0.2272\n",
      "Epoch 3/20\n",
      " - 52s - loss: 0.2245 - val_loss: 0.2183\n",
      "Epoch 4/20\n",
      " - 52s - loss: 0.2156 - val_loss: 0.2235\n",
      "Epoch 5/20\n",
      " - 52s - loss: 0.2090 - val_loss: 0.2106\n",
      "Epoch 6/20\n",
      " - 52s - loss: 0.2035 - val_loss: 0.2073\n",
      "Epoch 7/20\n",
      " - 52s - loss: 0.1984 - val_loss: 0.2081\n",
      "Epoch 8/20\n",
      " - 52s - loss: 0.1941 - val_loss: 0.2058\n",
      "Epoch 9/20\n",
      " - 52s - loss: 0.1900 - val_loss: 0.2068\n",
      "Epoch 10/20\n",
      " - 52s - loss: 0.1862 - val_loss: 0.2101\n",
      "Epoch 11/20\n",
      " - 52s - loss: 0.1828 - val_loss: 0.2042\n",
      "Epoch 12/20\n",
      " - 52s - loss: 0.1792 - val_loss: 0.2052\n",
      "Epoch 13/20\n",
      " - 52s - loss: 0.1763 - val_loss: 0.2049\n",
      "Epoch 14/20\n",
      " - 52s - loss: 0.1741 - val_loss: 0.2050\n",
      "Epoch 15/20\n",
      " - 52s - loss: 0.1708 - val_loss: 0.2039\n",
      "Epoch 16/20\n",
      " - 52s - loss: 0.1686 - val_loss: 0.2068\n",
      "Epoch 17/20\n",
      " - 52s - loss: 0.1663 - val_loss: 0.2064\n",
      "Epoch 18/20\n",
      " - 52s - loss: 0.1645 - val_loss: 0.2058\n",
      "Epoch 19/20\n",
      " - 52s - loss: 0.1626 - val_loss: 0.2079\n",
      "Epoch 20/20\n",
      " - 52s - loss: 0.1611 - val_loss: 0.2091\n",
      "9 validation loss: 0.20393357723357505\n",
      "40429/40429 [==============================] - 4s 89us/step\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "model_count = 0\n",
    "\n",
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    labels_train = labels[idx_train]\n",
    "    f_train = features_train[idx_train]\n",
    "\n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    labels_val = labels[idx_val]\n",
    "    f_val = features_train[idx_val]\n",
    "\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(f_train.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    \n",
    "    best_model_path = os.path.join(dataset + '-models', \"best_model\" + str(model_count) + \".h5\")\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([data_1_train, data_2_train, f_train], labels_train,\n",
    "                     validation_data=([data_1_val, data_2_val, f_val], labels_val),\n",
    "                     epochs=20, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=2)\n",
    "\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "    \n",
    "    # Predictions on leaved validation data # \n",
    "    leaved_valid_data_1 = data_1[valid.index]\n",
    "    leaved_valid_data_2 = data_2[valid.index]\n",
    "    f_leaved_valid = features_train[valid.index]\n",
    "    \n",
    "    preds = model.predict([leaved_valid_data_1, leaved_valid_data_2, f_leaved_valid], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    submission = pd.DataFrame({\"id\": valid[\"id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(os.path.join(dataset + \"-predictions\", \"preds\" + str(model_count) + \".csv\"), index=False)\n",
    "\n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
